{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "specified-daniel",
   "metadata": {},
   "source": [
    "# <center> Intersubject Generalization in Encoding Models Exploiting Deep Neural Netweorks <center/>\n",
    "    \n",
    "\n",
    "Welcome to this tutorial, demonstrating the cracks of our project decicated to developing the new methods of intesubject generalization!\n",
    "We will test different intersubject generalization algorithms (IGA) in the encoding model framework.\n",
    "    \n",
    "    \n",
    "This tutorial is a brief and shortened version of my Master's thesis aiming to give a reader the basic idea of the project. \n",
    "Many aspects of the original project will not be mentioned or explained in details here.\n",
    "\n",
    "    \n",
    "First, let us briefly introduce the problem we tackle in this project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-stations",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "### First of all, why are intersubject generalization algorithms (IGA) useful?\n",
    "The main reason for this is that *the brain responses of every subject to the presented stimuli are different due to subject-specific noise. IGA capture the underlying brain signal shared between the subjects (i.e. invariant of the subject-specific noise)*.\n",
    "\n",
    "There are multiple fields in congitive neroscience that may benefit from identifying the brain signal shared between the subjects. Some of the applications of the IGA are listed below.\n",
    "\n",
    "1. Transfer learning (e.g. no training session for online Brain-Computer Interface systems), (Cohen et al. 2017).\n",
    "\n",
    "2. Higher sensitivity of the statistical analysis, (Richard et al. 2020).\n",
    "\n",
    "3. Sceintific generality (conclusions about the underlying source), (Richard et al. 2020).\n",
    "\n",
    "\n",
    "### But what is the main idea behind IGA?\n",
    "The main objective of the IGA is to find a way to project the data from individual subject space into the space shared between all the subjects (figure below illustrates this concept). \n",
    "\n",
    "<img src=\"images/picture1.png\" width=700 height=700 />\n",
    "\n",
    "\n",
    "### Linear and non-linear IGA\n",
    "Based on the manner of projecting the data IGA can be divided into the ones using linear mathematical operations for transforming the data and the ones using deep neural networks for this purpose. We will refer to these groups as *linear* and *non-linear* IGA respectively. \n",
    "\n",
    "Linear IGA are commonly used whereas non-linear IGA are just beginning to emerge (the first and the only [paper](https://arxiv.org/abs/1608.04846) exploiting DNN for intersubject generalization we managed to find used Convolutional Autoencoder on fMRI data).\n",
    "\n",
    "### Idea\n",
    "Inspired by [SimCLR framework](https://arxiv.org/abs/2002.05709) recently proposed in machine learning in this project we aimed to develop non-linear IGA trained with contrastive loss, which would outperform the existing linear methods. \n",
    "\n",
    "We assume that the reader is already familiar with the original SimCLR framework. If it is not the case, feel free to check out the paper above.\n",
    "Let us say, we have a neuroimaging dataset collected while presenting multiple stimulito several subjects.\n",
    "To adapt SimCLR for this dataset, we treat the brain responses of different subjects to similar stimuli as positive samples and the responses of different subjects to different stimuli as negative samples. \n",
    "Then, for each stimulus we use the *encoder DNN* and shallow fully-connected *projection head DNN* to obtain some latent representation of the brain responses to positive and negative samples. The contrastive loss is then calculated between the outputs of projection head in such a way, that the brain responses for positive samples are as similar as possible and the brain responses for negative samples are as dissimilar as possible. \n",
    "\n",
    "<img src=\"images/picture2.png\" width=700 height=700>\n",
    "\n",
    "### DNN to train with contrastive loss\n",
    "We used the [Perceiver](https://arxiv.org/abs/2103.03206) as an encoder DNN. The Perceiver is an input modality independent DNN, which potentially allows us to use it with any neuroimaging data modality (EEG, fMRI, fNIRS, etc.).\n",
    "We used the [implementation of the Perceiver](https://github.com/lucidrains/perceiver-pytorch) by [Phil Wang](https://github.com/lucidrains).\n",
    "The figure below provides a scheme of the Perceiver architecture.\n",
    "\n",
    "<img src=\"images/picture6.png\" width=700 height=700>\n",
    "\n",
    "### Baselines\n",
    "As a baseline we will compare our contrastive loss-trained non-linear IGA with the state-of-the-art linear IGA and the first non-linear IGA.\n",
    "\n",
    "#### Multiview ICA\n",
    "[Multivview ICA](https://hugorichard.github.io/software/software-3/) is a recently proposed linear IGA which was shown to outperform the other frequently used algorithms. Therefore, we use it as a state-of-the-art linear IGA.\n",
    "\n",
    "#### Convolutional Autoencoder\n",
    "[Convolutional Autoencoder](https://arxiv.org/abs/1608.04846) is a simple DNN which was to the best of our knowledge the first  non-linear IGA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-librarian",
   "metadata": {},
   "source": [
    "## Dataset and Encoding model\n",
    "In this project we used EEG dataset recorded with Rapid serial visual presentation paradigm, (Grootswagers, Robinson, and Carlson 2019). \n",
    "Images from the [THINGS dataset](https://pubmed.ncbi.nlm.nih.gov/31613926/) grouped in train and test datasets were presented to 7 participants. EEG response to each image was recorded. \n",
    "The *encoding model* was constructed as follows. \n",
    "Each image was fed to [CORnet-S](https://arxiv.org/abs/1909.06161) DNN.\n",
    "The output activations of the CORnet-S (independent variable) and the EEG responses (dependent variable) to train images were used to train the linear regression. Then the output activations of the CORnet-S for the test images and trained linear regression were used to obtain the predicted EEG response. \n",
    "<img src=\"images/picture3.png\" width=900 height=900>\n",
    "\n",
    "The full technical description of the dataset is available in my master's thesis which will soon be made publically available.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-midnight",
   "metadata": {},
   "source": [
    "To assess the performance of the performance of the encoding model we used *generic decoding* procedure. I will explain it on the toy example below.\n",
    "<img src=\"images/picture4.png\" width=700 height=700>\n",
    "Let's say, we have computed the correlation between every predicted and real (recorded) EEG response and obtained a correlation matrix. Each row of the correlation matrix corresponds to the real image index and each column â€“ to the index of the predicted image. Next, for each row (real image) we sort the correlation values in descending order. So, for every real image we have the row of indices of the predicted images sorted by the strength of their correlation with the real one. After this we define the top value N = 2.  We count the number of rows, where the correlation value with the index equalling to the row number is less or equal to N. Finally, we divide this value into the number of images (4) and obtain top 2 accuracy.\n",
    "\n",
    "We ran the encoding model on the shared space response averaged over subjects and the indivudual shared space response (without averaging)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boolean-framework",
   "metadata": {},
   "source": [
    "Let us now summarize the full model pipeline.\n",
    "First, intersubject generalization was used to project individual subject data into the shared space.\n",
    "Second, the encoding model was run to obtained the predicted test set response.\n",
    "Third, the quality of the reconstruced EEG respnse was assessed via generic decding procedure.\n",
    "<img src=\"images/picture5.png\" width=900 height=900>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "communist-smith",
   "metadata": {},
   "source": [
    "## Intersubject generalization algorithms\n",
    "Now we come to the cracks of the project - intersubject genralization algorithms.\n",
    "\n",
    "As you remember, we wanted to develop non-linear IGA trained with contrastive loss, which would outperform the existing linear methods.\n",
    "Therefore, we compared the performance of the encoding model on the data transformed with the state-of-the art linear IGA, the Convolutional Autoencoder, and non-linear IGA trained with Contrastive loss (based on the Perceiver).\n",
    "As the control for the IGA we ran encoding model on the untransformed data.\n",
    "\n",
    "As the state-of-the art linear IGA we used recently proposed [MultiviewICA method](https://hugorichard.github.io/software/software-3/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corporate-phenomenon",
   "metadata": {},
   "source": [
    "## Running the experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legal-survivor",
   "metadata": {},
   "source": [
    "Here we reproduce some steps from the analysis. Not to overwhelm this tutorial with technical details, we skip many intermediate steps and leave the most essential ones. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-purse",
   "metadata": {},
   "source": [
    "First, set the *path* variable to the top directory of your project.\n",
    "In all the follow up code the path corresponds to the project directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "partial-visitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "path=/mnt/e/Documentos/Study/Master_thesis/intersubject_generalization/demo\n",
    "cd $path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raising-perth",
   "metadata": {},
   "source": [
    "In order to run the code you shall have all the packages from *requirements.txt* installed.\n",
    "One way to do it is to create an environment with the required packages (*conda create -n IGAdemo -f requirementx.txt*)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-parliament",
   "metadata": {},
   "source": [
    "### I. Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exposed-acrylic",
   "metadata": {},
   "source": [
    "1. Creating the *.pkl* dataset.\n",
    "The collected EEG is stored in the folder \n",
    "\n",
    "/data/EEG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introductory-establishment",
   "metadata": {},
   "source": [
    "Let us pack the data from multiple subjects into a single *.pkl* file for the train and test sessions.\n",
    "\n",
    "The file *create_dataset_matrix.py* packs the EEG data from different *.npy* files into a one *.pkl* file for train and test sets. The *-time* parameter corresponds to the EEG time window used in the analysis in samples, here we leave it 13 40 which corresponds to 60-600 ms after the stimulus presentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "significant-pixel",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-28aa1e02bf7d>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-13-28aa1e02bf7d>\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    cd code/prepare_data/\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "path=/mnt/e/Documentos/Study/Master_thesis/intersubject_generalization/demo\n",
    "\n",
    "cd code/prepare_data/\n",
    "eeg_dir=$path\"data/EEG/\"\n",
    "dataset_dir=$path\"data/datasets/\"\n",
    "python create_dataset_matrix.py -inp $eeg_dir -out $dataset_dir -time 13 40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charged-healthcare",
   "metadata": {},
   "source": [
    "2. Creating the feature matrix\n",
    "\n",
    "Next, we have to reshape the EEG dataset of shape *subjects x images x timepoints x channels* into the featurematix of shape *subjects x images x features*. This is the requirement of the [Multiview ICA toolbox](https://github.com/hugorichard/multiviewica/blob/master/multiviewica/_multiviewica.py) (feel free to see the documentation).\n",
    "\n",
    "The file *create_featurematrix.py* does the job.\n",
    "It accepts the directory where the EEG dataset files are stored and the output directory where the feature matrices will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "biblical-winning",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"create_featurematrix.py\", line 7, in <module>\n",
      "    import joblib\n",
      "ImportError: No module named joblib\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'cd code/prepare_data/\\nfeaturemat_dir=\"data/featurematrices/\"\\npython create_featurematrix.py -inp $dataset_dir -out $featuremat_dir\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-3e5b820f0019>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'bash'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cd code/prepare_data/\\nfeaturemat_dir=\"data/featurematrices/\"\\npython create_featurematrix.py -inp $dataset_dir -out $featuremat_dir\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2397\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2398\u001b[0m                 \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2399\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2400\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2401\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\IPython\\core\\magics\\script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[1;34m(line, cell)\u001b[0m\n\u001b[0;32m    140\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[1;31m# write a basic docstring:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-104>\u001b[0m in \u001b[0;36mshebang\u001b[1;34m(self, line, cell)\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\IPython\\core\\magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\IPython\\core\\magics\\script.py\u001b[0m in \u001b[0;36mshebang\u001b[1;34m(self, line, cell)\u001b[0m\n\u001b[0;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 245\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mCalledProcessError\u001b[0m: Command 'b'cd code/prepare_data/\\nfeaturemat_dir=\"data/featurematrices/\"\\npython create_featurematrix.py -inp $dataset_dir -out $featuremat_dir\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "path=/mnt/e/Documentos/Study/Master_thesis/intersubject_generalization/demo/\n",
    "cd code/prepare_data/\n",
    "\n",
    "dataset_dir=$path\"data/datasets/\"\n",
    "featuremat_dir=$path\"/data/featurematrices/\"\n",
    "python create_featurematrix.py -inp $dataset_dir -out $featuremat_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-climb",
   "metadata": {},
   "source": [
    "3. Creating the feature matrix with increment of the training data\n",
    "\n",
    "Linear methods are known to require less training data, than DNN-based. In orded to check how does the amount the the training data influence the performance of the IGA, we will create the feature matrices with sequentially increasing number of the train images. In order to do it we randomly select 10, 20, 40, 60, and 80 % of the training images. For reproducibility in the project we used the average performance over 10 random shuffles. However, in this tutorial we will only use 1 shuffle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skilled-tsunami",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "path=/mnt/e/Documentos/Study/Master_thesis/intersubject_generalization/demo/\n",
    "cd code/prepare_data/\n",
    "\n",
    "dataset_dir=$path\"data/datasets/\"\n",
    "incr_featuremat_dir=$path\"/data/incr_featurematrices/\"\n",
    "python create_featurematrix.py -inp $dataset_dir -out $incr_featuremat_dir -nsteps 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absent-stereo",
   "metadata": {},
   "source": [
    "Great! Now we have the feature matrices for the train and test set EEG responses and are ready to run intersubject generalization on it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "german-attendance",
   "metadata": {},
   "source": [
    "### II. Intersubject generalization\n",
    "Now let us run different intersubject generalization methods and see how it influences the performance of the encoding model.\n",
    "\n",
    "For every linear IGA we will:\n",
    "1. *Run the IGA on the EEG data;*\n",
    "2. *Run linear regression (average and subject-wise);*\n",
    "3. *Run generic decoding.*\n",
    "\n",
    "For non-linear IGA the same steps are integrated into the algorithm assessment pipeline (no need to explicitly follow these steps). \n",
    "\n",
    "In the end we will plot the generic decoding as bar plots for average and subject-wise data. This will allow us to compare the performance of the encoding model on the data transformed by each of the IGA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "green-differential",
   "metadata": {},
   "source": [
    "#### 1. Control\n",
    "In order to have a control for the IGA, we first run the encoding model on the untransformed EEG data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reserved-nigeria",
   "metadata": {},
   "source": [
    "##### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "italic-penguin",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-bash: line 8: unexpected EOF while looking for matching `\"'\n",
      "-bash: line 9: syntax error: unexpected end of file\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'cd code/linear/regression/\\nfeaturemat_dir=$path\"/data/source_data/featurematrices/\"\\ndnn_dir=$path\"/data/dnn_activations/\"\\n\\nfor regr_type in \"average\" \"subjectwise\"\\ndo\\n    python linear_regression.py -eeg_dir $featuremat_dir -dnn_dir $dnn_dir -regr_type $regr_type \\\\\\n    -out_dir $path\"/\\n'' returned non-zero exit status 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ae5c7595bcb6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'bash'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cd code/linear/regression/\\nfeaturemat_dir=$path\"/data/source_data/featurematrices/\"\\ndnn_dir=$path\"/data/dnn_activations/\"\\n\\nfor regr_type in \"average\" \"subjectwise\"\\ndo\\n    python linear_regression.py -eeg_dir $featuremat_dir -dnn_dir $dnn_dir -regr_type $regr_type \\\\\\n    -out_dir $path\"/\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2397\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2398\u001b[0m                 \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2399\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2400\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2401\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\IPython\\core\\magics\\script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[1;34m(line, cell)\u001b[0m\n\u001b[0;32m    140\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[1;31m# write a basic docstring:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-104>\u001b[0m in \u001b[0;36mshebang\u001b[1;34m(self, line, cell)\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\IPython\\core\\magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\IPython\\core\\magics\\script.py\u001b[0m in \u001b[0;36mshebang\u001b[1;34m(self, line, cell)\u001b[0m\n\u001b[0;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 245\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mCalledProcessError\u001b[0m: Command 'b'cd code/linear/regression/\\nfeaturemat_dir=$path\"/data/source_data/featurematrices/\"\\ndnn_dir=$path\"/data/dnn_activations/\"\\n\\nfor regr_type in \"average\" \"subjectwise\"\\ndo\\n    python linear_regression.py -eeg_dir $featuremat_dir -dnn_dir $dnn_dir -regr_type $regr_type \\\\\\n    -out_dir $path\"/\\n'' returned non-zero exit status 2."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "path=/mnt/e/Documentos/Study/Master_thesis/intersubject_generalization/demo/\n",
    "\n",
    "cd code/linear/regression/\n",
    "featuremat_dir=$path\"/data/source_data/featurematrices/\"\n",
    "dnn_dir=$path\"/data/dnn_activations/\"\n",
    "\n",
    "for regr_type in \"average\" \"subjectwise\"\n",
    "do\n",
    "    python linear_regression.py -eeg_dir $featuremat_dir -dnn_dir $dnn_dir -regr_type $regr_type \\\n",
    "    -out_dir $path\"/data/regression/control/\"\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-collins",
   "metadata": {},
   "source": [
    "##### Generic decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-malaysia",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "path=/mnt/e/Documentos/Study/Master_thesis/intersubject_generalization/demo/\n",
    "cd code/linear/generic_decoding/\n",
    "\n",
    "featuremat_dir=$path\"/data/source_data/featurematrices/\"\n",
    "pred_base=$path\"/data/regression/control/\"\n",
    "out_base=$path\"/data/generic_decoding/control/\"\n",
    "\n",
    "for regr_type in \"average\" \"subjectwise\"\n",
    "do\n",
    "    python generic_decoding.py -real $real_base\"/featurematrix_test.pkl\" \\\n",
    "    -pred $pred_base\"/Y_test_predicted_\"$regr_type\".pkl\" -d_type $regr_type \\\n",
    "    -out $out_dir $out_base\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limiting-eclipse",
   "metadata": {},
   "source": [
    "#### 2. Linear IGA \n",
    "We will test three linear IGA provided with the Multiview ICA package:\n",
    "* permica\n",
    "* groupica\n",
    "* multiviewica\n",
    "\n",
    "For the details on each method, please refer to the Multiview ICA [paper](https://arxiv.org/pdf/2006.06635.pdf).\n",
    "In the full version of the project we also selected the best preprocessing algorithm and the number of components to reduce the data to. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "later-detroit",
   "metadata": {},
   "source": [
    "##### Intersubject generalization\n",
    "\n",
    "This step may take a lot of time depending on the resources of your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-width",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "path=/mnt/e/Documentos/Study/Master_thesis/intersubject_generalization/demo/\n",
    "featuremat_dir=$path\"/data/source_data/featurematrices/\"\n",
    "cd code/linear/intersubject_generalization/\n",
    "\n",
    "# preprocessing details\n",
    "prepr=\"pca\"\n",
    "n_comp=200\n",
    "\n",
    "\n",
    "for method in \"multiviewica\" \"groupica\" \"permica\"\n",
    "do\n",
    "    echo \"Running \"$mehtod\"... This may take significant amount of time.\" \n",
    "    python linear_intersubject_generalization_utils.py -inp $featuremat_dir \\\n",
    "    -out $path\"/data/intersubject_generalization/linear/\"$method\"/\"\n",
    "    -dim_reduction $prepr -n_comp $n_comp\n",
    "    echo $method\" elapsed.\"\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adolescent-honey",
   "metadata": {},
   "source": [
    "##### Linear regression\n",
    "\n",
    "In the current version of the code, user shall specify the regression type (average or subject-wise) to the linear regression function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extraordinary-illustration",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "path=/mnt/e/Documentos/Study/Master_thesis/intersubject_generalization/demo/\n",
    "cd $path\"code/linear/intersubject_generalization/\"\n",
    "\n",
    "featuremat_dir=$path\"/data/source_data/featurematrices/\"\n",
    "dnn_dir=$path\"/data/dnn_activations/\"\n",
    "\n",
    "for regr_type in \"average\" \"subjectwise\"\n",
    "do\n",
    "    for method in \"multiviewica\" \"groupica\" \"permica\"\n",
    "    do\n",
    "        python linear_regression.py -eeg_dir $featuremat_dir -dnn_dir $dnn_dir -regr_type $regr_type \\\n",
    "        -out_dir $path\"/data/regression/linear/\"$method\"/\"\n",
    "    done\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portable-resource",
   "metadata": {},
   "source": [
    "##### Generic decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heated-browser",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "path=/mnt/e/Documentos/Study/Master_thesis/intersubject_generalization/demo/\n",
    "cd code/linear/generic_decoding/\n",
    "real_base=$path\"/data/intersubject_generalization/linear/\"\n",
    "pred_base=$path\"/data/regression/linear/\"\n",
    "out_base=$path\"/data/generic_decoding/linear/\"\n",
    "\n",
    "for method in \"multiviewica\" \"groupica\" \"permica\"\n",
    "do\n",
    "    for regr_type in \"average\" \"subjectwise\"\n",
    "    do\n",
    "        python generic_decoding.py -real $real_base\"/\"$method\"/shared_test.pkl\" \\\n",
    "        -pred $pred_base\"/\"$method\"/Y_test_predicted_\"$regr_type\".pkl\" \\ -d_type $regr_type \\\n",
    "        -out $out_dir $out_base$method\"/\"\n",
    "    done\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laden-discharge",
   "metadata": {},
   "source": [
    "#### 3. Non-linear IGA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cordless-pakistan",
   "metadata": {},
   "source": [
    "In the project we compared the performance of different DNN trained with SimCLR contrastive loss.\n",
    "We have also compared it the the Convolutional Autoencoder.\n",
    "In this tutorial we will only run *Perceiver* and *Convolutional Autoencoder* models with the best hyperparater and parameter configurations.\n",
    "Note, that if you do not have GPU on your device, the training processes may be quite slow (hours)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-universal",
   "metadata": {},
   "source": [
    "##### Convolutional Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funky-novel",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "path=/mnt/e/Documentos/Study/Master_thesis/intersubject_generalization/demo/\n",
    "cd $path\"/code/dnn/conv_autoencoder/\"\n",
    "\n",
    "eeg_dir=$path\"data/source_data/datasets/\"\n",
    "out_dir=$path\"data/intersubject_generalization/dnn/conv_autoencoder/\"\n",
    "\n",
    "# use -gpu flag if you have GPU on your device. It will make the training much faster.\n",
    "python convolutional_autoencoder.py -gpu -eeg_dir $eeg_dir -out_dir $out_dir -batch_size 16 -lr 0.01 \\\n",
    "n_epochs 20 -enc_chs 128 256 512 -dec_chs 512 256 -normalize 0 -scale 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extended-warrant",
   "metadata": {},
   "source": [
    "##### Perceiver\n",
    "\n",
    "For Perceiver it is recommended to use *-gpu* flag, since trainin on CPU may be very time-consuming.\n",
    "Feel free to check out the [page of the author](https://github.com/lucidrains/perceiver-pytorch) of the implementation of the Perceiver for more details about the parameters and hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-jurisdiction",
   "metadata": {},
   "source": [
    "First, you need to install the Perceiver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incoming-partition",
   "metadata": {},
   "outputs": [],
   "source": [
    " pip install perceiver-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-eleven",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "path=/mnt/e/Documentos/Study/Master_thesis/intersubject_generalization/demo/\n",
    "cd $path\"/code/dnn/conv_autoencoder/\"\n",
    "\n",
    "eeg_dir=$path\"data/source_data/datasets/\"\n",
    "out_dir=$path\"data/intersubject_generalization/dnn/perceiver/\"\n",
    "\n",
    "# Best parameters for Perceiver identified in this project\n",
    "lr=0.0001\n",
    "batch_size=128\n",
    "bpl=10\n",
    "epta=5\n",
    "n_epochs=1200\n",
    "clip_grad_norm=2\n",
    "\n",
    "perc_latent_array_dim=200\n",
    "perc_num_latent_dim=50\n",
    "perc_latent_head=2\n",
    "perc_depth=1\n",
    "out_dim_ENC=200 # as if mvica with PCA 200\n",
    "out_dim_PH=100 # as if mvica with PCA 100\n",
    "\n",
    "lar_ar_dim=200\n",
    "num_latent_dim=50\n",
    "depth=1\n",
    "out_dim_ENC=200\n",
    "out_dim_PH=100\n",
    "cross_head=1\n",
    "cross_dim_head=64\n",
    "latent_dim_head=64\n",
    "num_freq_band=12\n",
    "\n",
    "# use -gpu flag if you have GPU on your device. It will make the training much faster.\n",
    "\n",
    "python perceiver-projection_head-eeg-leftthomas.py -gpu -out_dir $out_dirs -batch_size $batch_size \\\n",
    "-lr $lr -bpl $bpl -epta $epta -n_epochs $n_epochs -perc_latent_array_dim $perc_latent_array_dim \\\n",
    "-perc_num_latent_dim $perc_num_latent_dim -perc_latent_head $perc_latent_head -perc_depth $perc_depth \\\n",
    "-out_dim_ENC $out_dim_ENC -out_dim_PH $out_dim_PH -perc_cross_heads $cross_head -perc_cross_dim_head \\\n",
    "$cross_dim_head -perc_latent_dim_head $latent_dim_head -perc_num_freq_bands $num_freq_band\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
